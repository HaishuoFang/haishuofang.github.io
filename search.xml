<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记一次多分类的炼丹过程]]></title>
    <url>%2F2018%2F09%2F09%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84%E7%82%BC%E4%B8%B9%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[emm… 这是一次从数据抓取到搭建模型，模型调参的全过程。 数据数据决定了模型的上限，这话一定要记着！！，多花点时间在数据上，要比上来一顿操作调参来的强。对于多分类问题，首先要观察的是数据是否干净、类别是否平衡。 一般需要通过上采样或者下采样的方法使样本尽可能的平衡 采用fscore 的评判标准，详细的看到每个类别的recall，precision 由于数据是我们从网站上爬下来的，所以存在很多标签标错的情况，当你用CNN，RNN的模型去训练的时候，你会发现自己的模型根本学不好，这时候你开始尝试各种模型，发现还是不行，然后各种debug，开始怀疑人生，怀疑自己，连一个分类都做不好！ 所以数据很重要。针对上述问题，最简单的方法就是误差分析，分析误差的来源，认真做好数据。 模型对于主题的文本分类来讲，CNN的效果已经是公认的了，可以并行、收敛速度快，效果好。 1.shadow CNN 也是一种ngram的特征抽取，通过卷积操作提取特征，通过maxpooling抽取关键特征，通过全连接层挖掘关键特征之间的关系，最后通过softmax进行分类。 2.deep CNN,2017 ACL的一篇文章《Deep Pyramid Convolutional Neural Networks for Text Categorization》 主要思想是在shallow CNN 的基础上进行关键特征之间的特征组合，使得模型不仅仅是一个ngram的关键特征模型，更能够掌握语义信息，句法信息等。并且作者提出了先做activation再做线性运算，并且加了short cut以防止较深层次的CNN在nlp方面不work,难训练的问题。原文中作者使用的固定的feature map,这样的话，保证尺寸不变可以不断的压缩左右两边各(n-1)/2的特征到同一个位置，然后，通过stride为2的maxpooling 不断的降维，这样期望获得全局强特征，并且可以降低训练难度。当然可以加入batch norm进去，我自己做了实验发现加入batch norm 还是有了提升。 3.RCNN，RCNN也是期望通过结合CNN特征抽取优点和RNN的时序特点，希望获得对分类有益的全局特征。 这边的卷积层或者叫做特征提取层是以recurrent的形式，拼接当前的embedding，并通过全连接进行的。这个方法进行了RNN的操作，如果文本很长时，收敛较慢。 4.BiLSTM，RNN模型对于主题文本分类这种情况来说，效果不会差也不会很好，对于一些时序信息较强的任务，例如分词、实体识别，RNN还是可以胜任的。 模型训练及调参 对于不同的激活函数选择不同的参数初始化的方式，比较重要的一点就是方差设置的不能太大，也不能太小，比如Tanh，如果方差设置太大会造成梯度饱和，更新缓慢。 优化方法的选择：一般没有什么把握的选择自适应的Adam，但我在rcnn上测试，用SGD配合lr 从1.0或者0.1开始衰减，会达到比Adam较好的结果。 针对于RNN的模型可以尝试加入gradient_clip，会有较好的效果，对于CNN的模型加入batch norm，实测有好的效果。 还有一些对于embedding_size,hidden_size,batch_size的调参可以去尝试，在可训练的embedding中，可以去比较200dim、100dim对效果的影响。 针对过拟合问题，这个问题大家都很懂了，可以设置min_count,l2正则,dropout，停用词和标点符号是否需要去除得看具体问题，复杂模型可以换成简单模型。 针对到底是使用字还是词，个人感觉如果分词器分不准的话还不如用字，端到端的还是可以学习到。 最重要的一点，模型的复杂度要和数据匹配！！！，有的时候只需要一层cnn，甚至激活函数都不要，再加个分类层，一个简单的线性模型，就可以达到很多复杂模型达不到的效果。]]></content>
      <categories>
        <category>分类</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>多分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从HMM推到CRF]]></title>
    <url>%2F2018%2F09%2F09%2F%E4%BB%8EHMM%E6%8E%A8%E5%88%B0CRF%2F</url>
    <content type="text"><![CDATA[知乎旧稿本文主要是从HMM到CRF的推导，对李宏毅老师的课作个备忘。 HMM的二个假设: 齐次马尔科夫性假设：P(i_{t}|i_{t-1},o_{t-1}...,i_{1},o_{1})=P(i_{t}|i_{t-1}), t=1,2...T 即t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关，并且与时刻t无关。 观测独立性假设：P(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{1},o_{1})=P(o_{t}|i_{t}) 即t时刻的观测仅与t时刻的状态。 HMM 是生成式模型，所以是对P(x,y)联合概率分布进行建模，针对具体问题： 其中，y是状态变量，x是观测变量，根据联合概率的公式可以写成P(x,y)=P(y)P(x|y)。 根据二个假设可以展开写成： 蓝色下划线代表的状态转移的那一部分的概率，红色部分表示的是发射概率，可以看出HMM的状态转移和发射部分是分开建模的，crf是有特征函数，将这两个合在一起建模的，这是crf的好处。 从HMM到CRF对P(x,y)取对数： 其中可以进行改写： P(t|s)表示标注s 生成word t的概率，N(s,t)表示标注 s生成word t的次数，已经很像crf中特征函数和权重的相乘了。 此处是一个例子： 对每一项都进行这样的改写，可以得到： 这样，此处的对数概率就变成了权重， N_{s,t}(x,y) 表示成了特征函数，对于crf来说可以不仅仅是由动词后面接名词这种特征，它可以考虑ngram的特征，还有一些其他自己定义的特征，然后去求解对应的权重。 这样P(x,y)写成下面的式子： crf的公式就可以表示出来： 将 $w.\phi(x,y)$ 展开可以写成标准的crf公式： 其中 $t_{k}$ , $s_{l}$ 是特征函数， $\theta_{k}$, $\theta_{l}$ 是权重。 这样由HMM导出了CRF。对于crf的训练，自然是更新每种特征的权重，使得表现最好，使得P(y|x)最大, 最终求导得到的公式如下： 可以利用此导数对不同的特征进行权重的更新。]]></content>
      <categories>
        <category>序列标注</category>
      </categories>
      <tags>
        <tag>hmm</tag>
        <tag>crf</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rnn基本模型总结]]></title>
    <url>%2F2018%2F09%2F09%2Frnn%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[整理知乎上的旧稿。 此篇是根据ng的deeplearning.ai的序列模型做的笔记，主要是为了总结一下RNN的基本的序列模型。还望大神勿喷。看完这篇文章，对于TextRNN,RCNN,Hierarchical Attention Network等模型结构就更好理解了，这些结构无非就是基本RNN结构的拼接，转换。 我认为RNN模型有以下三个特点： 时序性，递归性$y_{t} = f(y_{t-1},x_{t})$ ,一般输入来自两个方面，一个是之前状态 $y_{t-1}$ ，和当前状态的输入 $x_{t}$ 。 参数共享，每一步的参数矩阵都是共享的，主要的参数矩阵也是上述的两个方面， $W_{aa}$ 和 $W_{ax}$ 。 cell的设计，为了解决rnn更新时指数式的梯度弥散，梯度爆炸的问题和控制cell保留信息比例的问题，设计了gru,lstm cell，具体见后面。 反向传播过程 从图中可以看出，参数的更新方式也是基于时间步的，从loss function开始求导，最终都汇总的输入刚开始的时间步开始更新，由于每一步的参数都是共享的，更新了最初的时间步的参数，也就更新了所有的参数。这是RNN和其他网络结构不同的地方。 GRU GRU cell有两个gate: update gate:顾名思义，就是更新cell的gate,不同于lstm,利用forget gate和input gate分别去控制当前输入 $\tilde{c}^{t}$ 和之前cell $c^{t-1}$，gru只用了一个update gate。 reset gate:控制当前输入 $\tilde{c}^{t}$ 中有多少来自之前的输出，取值在0~1之间。 GRU模型并没有ouput gate,$a^{t}$ 与cell中保存的信息 c^{t} 是一样的。 LSTM LSTM相较于GRU多了一个gate,即forget gate,将GRU中的update gate分成input gate和forget gate。且输出信息通过output gate去控制。 input gate (图中 $\Gamma_{u}$ ) : 控制当前输入所占比例 forget gate (图中 $\Gamma_{f}$ ): 控制之前cell所占比例 output gate(图中 $\Gamma_{o}$ ) : 控制输出所占比例通过gate的设计我们可以根据不同的任务学得不一样的参数，以控制当前信息和之前cell中的信息保留的比例，去更好的完成时序任务，同时也能防止梯度的弥散和爆炸（gradident clip 可以解决这个问题） 双向RNN 双向RNN意味着对于一条语句我们可以正着输入一遍，反过来输入一遍，这样对于每一个sate我们既可以考虑到前面的信息也可以考虑到后面的信息，然后concat二者的output 接全连接层即可得到输出。 deep rnn Deep RNN用多层的cell去提取特征，相当于cnn里面设置的多个卷积核。这样提取特征的效果会更好。]]></content>
      <categories>
        <category>rnn基本原理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>rnn</tag>
      </tags>
  </entry>
</search>
