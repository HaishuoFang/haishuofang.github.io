<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从HMM推到CRF]]></title>
    <url>%2F2018%2F09%2F09%2F%E4%BB%8EHMM%E6%8E%A8%E5%88%B0CRF%2F</url>
    <content type="text"><![CDATA[知乎旧稿本文主要是从HMM到CRF的推导，对李宏毅老师的课作个备忘。 HMM的二个假设: 齐次马尔科夫性假设：P(i_{t}|i_{t-1},o_{t-1}...,i_{1},o_{1})=P(i_{t}|i_{t-1}), t=1,2...T 即t时刻的状态只依赖于前一时刻的状态，与其他时刻的状态和观测无关，并且与时刻t无关。 观测独立性假设：P(o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},...,i_{1},o_{1})=P(o_{t}|i_{t}) 即t时刻的观测仅与t时刻的状态。 HMM 是生成式模型，所以是对P(x,y)联合概率分布进行建模，针对具体问题： 其中，y是状态变量，x是观测变量，根据联合概率的公式可以写成P(x,y)=P(y)P(x|y)。 根据二个假设可以展开写成： 蓝色下划线代表的状态转移的那一部分的概率，红色部分表示的是发射概率，可以看出HMM的状态转移和发射部分是分开建模的，crf是有特征函数，将这两个合在一起建模的，这是crf的好处。 从HMM到CRF对P(x,y)取对数： 其中可以进行改写： P(t|s)表示标注s 生成word t的概率，N(s,t)表示标注 s生成word t的次数，已经很像crf中特征函数和权重的相乘了。 此处是一个例子： 对每一项都进行这样的改写，可以得到： 这样，此处的对数概率就变成了权重， N_{s,t}(x,y) 表示成了特征函数，对于crf来说可以不仅仅是由动词后面接名词这种特征，它可以考虑ngram的特征，还有一些其他自己定义的特征，然后去求解对应的权重。 这样P(x,y)写成下面的式子： crf的公式就可以表示出来： 将 $w.\phi(x,y)$ 展开可以写成标准的crf公式： 其中 $t_{k}$ , $s_{l}$ 是特征函数， $\theta_{k}$, $\theta_{l}$ 是权重。 这样由HMM导出了CRF。对于crf的训练，自然是更新每种特征的权重，使得表现最好，使得P(y|x)最大, 最终求导得到的公式如下： 可以利用此导数对不同的特征进行权重的更新。]]></content>
      <categories>
        <category>序列标注</category>
      </categories>
      <tags>
        <tag>hmm</tag>
        <tag>crf</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rnn基本模型总结]]></title>
    <url>%2F2018%2F09%2F09%2Frnn%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[整理知乎上的旧稿。 此篇是根据ng的deeplearning.ai的序列模型做的笔记，主要是为了总结一下RNN的基本的序列模型。还望大神勿喷。看完这篇文章，对于TextRNN,RCNN,Hierarchical Attention Network等模型结构就更好理解了，这些结构无非就是基本RNN结构的拼接，转换。 我认为RNN模型有以下三个特点： 时序性，递归性$y_{t} = f(y_{t-1},x_{t})$ ,一般输入来自两个方面，一个是之前状态 $y_{t-1}$ ，和当前状态的输入 $x_{t}$ 。 参数共享，每一步的参数矩阵都是共享的，主要的参数矩阵也是上述的两个方面， $W_{aa}$ 和 $W_{ax}$ 。 cell的设计，为了解决rnn更新时指数式的梯度弥散，梯度爆炸的问题和控制cell保留信息比例的问题，设计了gru,lstm cell，具体见后面。 反向传播过程 从图中可以看出，参数的更新方式也是基于时间步的，从loss function开始求导，最终都汇总的输入刚开始的时间步开始更新，由于每一步的参数都是共享的，更新了最初的时间步的参数，也就更新了所有的参数。这是RNN和其他网络结构不同的地方。 GRU GRU cell有两个gate: update gate:顾名思义，就是更新cell的gate,不同于lstm,利用forget gate和input gate分别去控制当前输入 $\tilde{c}^{t}$ 和之前cell $c^{t-1}$，gru只用了一个update gate。 reset gate:控制当前输入 $\tilde{c}^{t}$ 中有多少来自之前的输出，取值在0~1之间。 GRU模型并没有ouput gate,$a^{t}$ 与cell中保存的信息 c^{t} 是一样的。 LSTM LSTM相较于GRU多了一个gate,即forget gate,将GRU中的update gate分成input gate和forget gate。且输出信息通过output gate去控制。 input gate (图中 $\Gamma_{u}$ ) : 控制当前输入所占比例 forget gate (图中 $\Gamma_{f}$ ): 控制之前cell所占比例 output gate(图中 $\Gamma_{o}$ ) : 控制输出所占比例通过gate的设计我们可以根据不同的任务学得不一样的参数，以控制当前信息和之前cell中的信息保留的比例，去更好的完成时序任务，同时也能防止梯度的弥散和爆炸（gradident clip 可以解决这个问题） 双向RNN 双向RNN意味着对于一条语句我们可以正着输入一遍，反过来输入一遍，这样对于每一个sate我们既可以考虑到前面的信息也可以考虑到后面的信息，然后concat二者的output 接全连接层即可得到输出。 deep rnn Deep RNN用多层的cell去提取特征，相当于cnn里面设置的多个卷积核。这样提取特征的效果会更好。]]></content>
      <categories>
        <category>rnn基本原理</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>rnn</tag>
      </tags>
  </entry>
</search>
